# 프로젝트

프로젝트 만든 것이 중요한게 아니라 얼마나 고민하고 얼마나 이해하였는지. Node를 사용했다면 Node를 왜 썼는지

당연하지만 궁금할법한 것을 물어봄

특정 기술보다는 제너럴한 기술.

## 야나독

## kucctimetable

- 시간표 등록시 두 번 확인
- CICD travis 이용

## airbnb clone project

### 기술적 문제 및 해결

> 1. 모달창이 컴포넌트별로 있을 때의 문제와 그 해결  
>    각각의 컴포넌트가 개별 모달창을 가지고 있을 때, 모달 창이 여러개 나오는 경우가 발생할 수도 있다.  
>    상위 컴포넌트에서 모달을 관리하고 하위 컴포넌트에게 클릭 이벤트의 결과를 받아서 모달 창을 띄워주는 방식으로 변경
> 2. Apollo의 Hook인 useQuery. variable이 변경될 때마다 요청이 다시 보내지는 것을 몰랐다. 결국 axios 변경해서 프로젝트 수행

### 기술적 도전

nGinX 를 이용하여 로드 밸런서와 리버스 프록시 구현

## youngstargram

### 기술적 도전

- 성능 최적화
  - 무한 스크롤
  - React-virtualized
  - 쿼리 최적화
  - 캐싱

# 기술적으로 힘들었던 점

### 페이지네이션

- 데이터 베이스에 요청

### 질문 1. Big N 개의 팔로우

> 1000명을 팔로우 하고 있는 경우를 가정해보자. 우선 팔로우 테이블 에서 내가 팔로우하고 있는 사용자의 리스트를 구해야 한다. 그리고 게시글 테이블에서 위 리스트를 where 절에 넣어서 검색을 한다. 전체 사용자가 지나치게 많아 게시글 테이블에 게시글이 너무 많고 내가 팔로우 하고 있는 사람들이 너무 많을 경우에 과부하가 걸리지 않는가?
>
### 답

> 해결 하기 힘든 문제이다. 실제로 페이스북은 이 문제를 해결하기 위해서 운영체제를 튜닝해서 사용하고 있다. 이러한 문제는 사용자가 10만명 이상 있어야 겨우 지연이 보이는 수준이기 때문에 현 단계에서는 고려할 필요가 없다.

> 사용자가 늘어나면 이런부분을 튜닝을 하겠다. 학습을 한다. 구현은 현재 상황에 맞게 나중에 스트레스가 올라갔을 때 나중에 유저가 많아지는 상황에 당면했을 때 어떻게 대응할건지 어떻게 문제를 해결할 것인지를 고민하고 해당 내용을 포함하라.

### 질문 2. 페이지네이션 질문

> limit와 offset을 지정해서 페이지네이션을 할 때에, 매번 orderby를 이용하여 테이블을 정렬하는가. 

### 답
> 애당초 orderby 와 unique와 같은 속성은 지양하는 것이 좋다. orderby가 있는 쿼리는 잘못 짠 쿼리이다. 데이터 베이스에는 row가 시간 순서대로 들어가기 때문에 시간을 기준으로 order를 할 필요가 없다.
>

### 질문 3. 페이지네이션 원리
> limit를 사용할 경우, limit 만큼의 데이터만 찾고 search를 끝내는가
>
### 답
> 그렇다. 그렇기 때문에 더더욱 order by를 하면 안된다. order by를 할 경우에는 DB 전체의 테이블을 정렬하고 상단부터 limit개 만큼 가져오기 때문에 그 비용이 지나치다. order by가 들어가 있으면 기본적으로 잘못된 쿼리이다.

>  레코드를 쌓으면 시간의 역순으로 쌓이게 되어있다. 이를 이용하여 할 수 도 있다. 하지만 실제로 DB에 값이 들어갈 때 정확한 시간 순서가 보장되지 않을 수도 있다. 

> 하지만 index를 사용하면 괜찮다. index를 사용하여 order by를 하면, 이미 정렬이 되어 있기 때문에 전체 테이블을 검색하지 않는다.

> If) User 테이블에 Index를 설정하는데 User를 검색할 때에는 `길이순`으로 검색하고 싶을 경우에는 Index가 의미가 있을까? => 의미가 없다.

> If2) User가 사전 순서대로 b tree가 들어갈 때에 검색을 역 사전 순으로 하면  index의 효율성이 유지가 되는가? -=> 그렇다.

쿼리플랜 explain db에 엑세스하는 건 무조건 적어야한다. 인덱스를 걸어야한다. 보는게 중요한 서비스이기떄문에 읽기 최적화에 집중해야한다. 캐시를 하는게 좋다. 레디스랑 mysql 속도가 1000배쯤 차이가 난다. 인메모리 캐시같은걸 잘 활용해서 하면 순식간에 가져올 수 있다.

### 질문 4. 밀리 세컨드까지 같을 경우에 순서를 어떻게 보장할 것인가.

### order by를 두 번하는 방법
>  updatedAt만으로 정렬을 하는 것이 아니라 ['updatedAt', 'PK']로 정렬을 하는 방법.
> 정렬을 두번 한다는 점에서 비효율적이다.

### cursor에 lte를 적용하는 경우
>  현재 커서에서는 로드된 마지막 게시글의 작성 시간이 들어가있다. 현 방식은 cursor 값에 Op.lt를 적용하여 마지막 게시글 이후의 게시글 중에서 10개를 가져온다. 즉 모든 게시글의 작성 시간 값은 unique 하다는 것을 전제로 한다.
>
> lte 방식을 이용하면, 마지막 게시글의 작성 시간이하(해당 시간 포함 및 이전 시간) 포함한 게시글들을 요청한다. 클라이언트에서 겹치는 게시글을 제외하고 렌더링 한다. 이 방식은 시간이 겹치는 게시글이 limit개 미만일 경우에만 정상 작동한다. limit개가 넘을 경우 cursor 값이 변경되지 않아 해당 시간의 게시글만 무한히 요청된다.



## 캐싱 이슈

아폴로 캐싱

cache-first, cache-and-network

무한 스크롤 할 때 요청이 연속으로 보내지는 버그



## 무한 스크롤

요청에 대한 결과가 돌아와서 loading이 false가 되었는데, 결과가 들어와서 캐시가 변경되기 전에 요청을 다시 보내게 되면 캐시 변경전을 기준으로 요청을 보내기 때문에, 중복된 요청이 날아감.

### 해결

- 요청을 보내는 즉시 observer를 disconnect & 응답이 와서 re-rendering이 될 때, 다시 observer 등록 => 데이터를 2회 이상 보내지 않음.
- 요청을 보내는 곳을 한 군데로 고정 => 요청 두개를 빠르게 보낼 경우 위의 문제가 발생할 수 있음. 한군데로 바꾸지 않고 위의 방식을 적용하면, 하나의 요청이 보내지는 즉시 observer 가 모두 해제 되기 때문에 그 다음 요청은 보내질 일이 없음.





인스타그램 모바일웹으로 확인해볼 것. 

우리팀 선회한 방향이 좋은 방향인지?

크롱님은 괜춘 

최적화라는게 기능이 많아지고 복잡해질 때 의미가 있는건데 그래서 지금 단계에서 그걸 고민하는게 맞는지 모르겠다. 그거에 대해서 고민해보면 좋겠다.
핵심기능이 작동하는 상황에서는 좋다고 생각한다. 

### 팀플을 하면서 힘들었던 점
### 기술적으로 자랑할만한 점
  - 무한 스크롤
### 그냥 자랑할만한 점
### 팀플 진행 방식

- workflow: git flow 를 사용- 설명



# 교육

## 삼성 SDS 알고리즘 특강

- 무슨 교육?
- 자신있는 알고리즘 하나 설명: union-find

## 부스트 캠프 챌린지

- 무슨 교육?

## 부스트 캠프 멤버십

- 무슨 교육?

# 전공

## 자료구조

## DB

## 운영체제

## 컴퓨터 구조

## 네트워크





