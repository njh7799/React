# 질문 템플릿

What & Why & How

## 프로젝트 개요

- 목적
- 결과
- 동작 원리간단 설명
- why 기술
- why 구조

## 기술 스택

- 왜 그 기술 스택
- 기술 스택 특징
- 경쟁자와 차이점

## 무슨 일을 맡았는가? (어디에 중점을 두었는가?)

- 이유는?
- 그래서 무슨 성과를 내었는가?
- 무엇을 배울 수 있었는가?

## 테스트



## 더 개선하고 싶은 점이 있는가?

- 추가하고 싶은 기능

# SuperFastHTMLServingSystem

프로젝트 구조와 원리 간단하게 설명

## 기술 스택

- React
- Typescript
- Webpack
- Babel
- SSR

## 프로젝트를 선택 이유

### 왜?

첫 번째 이유는 한 번도 접해 보지 못한 구조를 설계할 수 있는 기회였기 때문이다. 이 때까지 제가 설계하고 구현해본 페이지들은 클라이언트와 단일 api 서버로 이루진 것이 대부분이었다. 하지만 이 프로젝트의 구조는 하나의 메인 페이지와 그 페이지에 컨텐츠 제공하는 컨텐츠 서버로 나누어져 있었다. 대규모 포탈에서는 이미 사용하고 있는 방식임에도 직접 구현해볼 기회가 없었던 것을 작은 규모로나마 설계하고 구현해 볼 수 있다는 것이 기대되었다. 두 번째 이유는 성능 최적화 및 사용자 경험 개선에 초점이 맞추어진 프로젝트였기 때문이다. 나는 평소에 사용자 경험 개선 및 성능 최적화에 관심이 많았다. "Super Fast HTML Serving" 이라는 이름을 보고 사용자 경험 개선에 대해 많은 공부를 할 수 있을 기회라고 판한다여 선택하였다.

### 얻고자 했던 것

포탈 서버 페이지에 대한 이해. 성능 최적화 

### 얻은 것

- SSR에 대한 이해
  - 원리: 서버에서 리액트 스크립트를 미리 읽어서 템플릿만 제작. 이를 클라이언트에게 서빙. 해당 템플릿에는 CSR 코드가 포함되어있기 때문에, 뒤에서 CSR 코드가 돌고 SSR 템플릿을 대체한다.
  - 한계 1: 초기 SSR 렌더링 후 CSR 코드가 돌기 전에는 상호작용 불가
  - 한계 2: SSR 단계 코드와 CSR 코드의 강제 동기화. SSR 단계에서 코드 내에 별도의 기능을 추가해도 hydrate 메소드가 실행 되면서 다 지워 버리고 순수한 CSR 코드로 덮어버린다. 
- 다양한 구조의 가능성와 장단점: 내가 시도했던 다양한 방식들
- 리액트에 대한 보다 깊은 이해

### 얻지 못한 것

성능 최적화에 대한 더욱 다양한 시도 및 측정 (컨텐츠가 너무 가벼워서 측정 실패) & 너무 기본적인 것들만 적용

### 왜 얻지 못했나

구조를 설계하는데 너무 많은 시간을 투자했다.

## 프로젝트에 대해 설명해봐라

### SSR을 이용한 슈퍼패스트 HTML 서빙? 어떻게 구현했나?

컨텐츠 서버는 express 와 pug 를 이용한 MPA 구조로 설계하였습니다. 요청이 들어오면 url에 따라 해당 컨텐츠의 마크업을 응답해주는 방식입니다. 메인 페이지는 리액트를 기반으로 설계하였습니다. 메인 페이지에 접근하면  주 서버에서`ReactDOMServer.renderToString` 메소드를 이용하여 마크업을 제작합니다. 그리고 컨텐츠 서버로 요청을 보내어 컨텐츠들을 응답 받습니다. 응답받은 컨텐츠를 이전 단계에서 제작한 마크업에 삽입합니다. 마지막으로 완성된 마크업을 클라이언트에게 응답합니다.

### 어떻게해서 이 구조에 도달했는가?

1. iframe: iframe은 내부 페이지의 처리가 모두 완료될 때까지 렌더링을 중지시킨다.
2. React + React: 코드의 크기가 너무 커진다, 충돌이 발생할 수 있다. 특정 라우터의 특정 컨텐츠만 가져오는 것은 불가능하다.
3. React + vanilla(template): 오류 검증 힘듦 디버깅 힘듦 하나의 파일에 다 때려 넣어야 함.
4. React + express pug
5. 최종 구조

### 어떤 컨텐츠를 개발했는가?

계산기와 시계를 구현했습니다. 특히 계산기는 수식 파싱 알고리즘을 적용하여 구현하였습니다

### 진행 방식

첫 번 째로 계획과 실행입니다. 저는 두 달간의 마일스톤을 주단위로 설계하였습니다. 그리고 매주 일 단위로 업무 계획을 세워 실행하였습니다. 그 때 그 때 학습한 내용들과 고민거리, 해결 과정들을 주간 업무 일지에 일단위로 기록하며 정리를 해나갔습니다. 매주 주간발표를 가져 현재 진행 상황과 현재 제가 설계하고 있는 구조에 대해 발표하고 피드백을 받으면서 새로운 구조를 설계하거나 현재 구조를 개선하는 방식으로 진행하였습니다. 



## 힘들었던 점

SuperFast HTML Serving 하는 프로젝트를 만들어야 한다는 목표만을 가지고 모든 설계를 직접 해야하여 힘들었습니다. 구조에 대해 팀원분들의 피드백을 받긴 했지만, 어디까지나 제가 설계한 구조에 대한 피드백만들 받았을뿐 정답을 알려주시지는 않았습니다.

## 어디에 중점을 두었는가

- 어디에 중점을 두었는가?
- 자랑할만한 부분이 있나? 어필하고 싶은 기능 설명해 봐라
- 성능 최적화에 중점을 뒀다고 했는데, 뭘 적용 시켰는가

## 성능 개선

### 어떤걸 개선했나?

1. 웹팩으로 정적 파일 압축
2. 내장 폰트 사용
3. 정적 파일 preload 적용
4. 정적 파일들 위치 변경하여 HTML 블로킹 최소화
5. HTTP Keep alive 옵션 적용

1. 컨텐츠 돔 접근 최소화
2. 계산기 수식 파싱 알고리즘

### 성능 측정은 어떻게 했나?

크롬 개발자 도구

## 왜 그런 구조를 선택했는가?

### 프론트는 왜?

- 프론트는 왜 리액트를 사용했나
- 왜 CRA 안썼나: 사용하지 않는 파일을 제거하여 용량을 줄이기 위해
- 왜 Next 안썼나: 프레임워크는 구조를 강제하기 때문에 내가 원하는 구조를 설계할 수 없다.
- 왜 웹팩 직접 빌드 했나
- 이 구조의 장단점은

### 컨텐츠 서버는 왜?

- 백엔드 왜 최신 문법 안쓰고 MPA 썼나
- 이 구조의 장점
- 단점 및 Plan B

## 아쉬웠던 점

### 성능 측정 방식

성능 측정을 크롬 개발자 도구밖에 사용을 안했다. 다른 툴이나 방법을 사용해보았으면 좋았을 것 같다.

### 브라우저 호환성

크롬에서 밖에 확인을 안했다. IE, 사파리 등 다른 브라우저와의 호환성도 확인을 했으면 좋았을 것 같다.

### 정형화된 컨텐츠 템플릿

컨텐츠 서버의 형식이 어느정도 강요가 된다. 처음에는 어디에서나 간편히 삽입하여 사용할 수 있는 컨텐츠 서버를 구현하고 싶었는데, 최적화를 진행하다 보니 어느정도 구조가 강제 되었던 것 같다. 구조를 강제할 계획이었다면 처음부터 확실하게 구조를 설계하고 static 서버에 띄워서 가져 가는 식으로 구현했으면 어땠을까 싶다. 

## 추가하고 싶은 기능

### 이미지 컨텐츠와 애니메이션
갤러리와 같은 이미지 컨텐츠를 만들어보고 싶다. 이미지에 관련한 성능 최적화 방법이 상당히 많은 것으로 알고 있는데, 이번 기회에 적용해보지 못하여 아쉽다.

### 상태 관리

시계를 예로 들자면 어떤 지역을 기준으로 시계를 보여줄 것인지 고를 수 있도록 설정하는 것이다.

### 초기 값 부여
완성한 구조는 요청을 보냈을 때 같은 종류의 응답만 온다. 요청에 쿼리를 추가하여 쿼리에 따라 초기 값이 다르게 설정되는 기능을 추가하고 싶다. 가령 시계의 경우 런던 시계를 요청할 경우 런던의 시계로 초기화가 되는 식이다.
### 캐시 서버 구현

컨텐츠 서버와 메인 서버 사이에 캐시 서버를 두고 자주 오는 요청을 캐싱 하여 즉각적으로 돌려주는 구조이다. (운영체제 캐시 부분 참고)

# Youngstargram  

## 기술 스택

- React
- Express(NodeJS)
- GraphQL
- MySQL

## 어디에 중점을 두었나?

성능 최적화

### 성능 측정을 무었을 기준으로 했는가?

크롬 개발자 도구를 사용했다. network, performance, audit 탭을 사용하였으며 performance 탭을 가장 많이 사용했다. FP, load 이벤트를 중점적으로 보았다.



### 메인 페이지 캐싱 전략 cach-first

- 이 부분은 인스타그램을 밴치마킹하여 정한 전략이다.

- 현재 타임 라인의 캐싱 전략은 캐시가 없을 경우에만 요청을 보내고 캐시가 있을 경우에는 캐싱된 값을 보여준다. 이 방식을 취함으로써 요청을 줄일 수 있다.

- 팔로우를 취소한다면?

  - 팔로우를 취소할 때, 취소한 유저의 게시글을 메인 페이지 캐시에서 filtering 을 통해 지워준다.

- 팔로우를 한다면?

  - 새롭게 팔로우를 한다면, 메인 페이지 캐시에는 아무런 변화도 주지 않는다. 현재 캐싱된 값들은 사용자가 메인 페이지에 접속하기 직전에 게시된 포스트들일 것이다. 또한 새롭게 팔로우한 인물의 게시글들 중 가장 오래된 게시글이 내 메인 페이지에 캐싱된 값들보다 늦은 시간에 게시되었을 가능성은 크지 않다. 따라서 메인 페이지로 돌아가 무한 스크롤을 계속 하다 보면 새롭게 팔로우한 인물의 게시글이 보일 것이다. 물론 정말 우연히 내가 팔로우 하기 직전에 게시글이 추가되었을 수도 있지만, 이 경우까지 고려하는 것은 낭비라고 판단하였다.

  

### 무한 스크롤 적용

- 페이지네이션

### windowing 기법 적용

  - 초기 렌더링 속도를 개선할 수 있었다. `display:none` 설정을 추가 하여도 초기 렌더링 속도가 크게 차이가 없다는 점(2.7 -> 2.6)에서, 초기 돔 트리를 구성하는 부분에서 병목 현상이 발생하였음을 알 수 있었다.

## 안한 것

### 실시간 동기화

  - 팔로워가 게시글을 작성할 때 마다 피드를 업데이트 시키면 오버헤드가 엄청날 것이다. 인스타그램의 특성상 새로운 게시글이 초단위로 올라오지 않기 때문에, 실시간 업데이트가 주는 사용성 증가와 실시간 업데이트를 적용함으로써 느려지는 사용성 저하를 비교하였을 때 후자의 단점이 전자의 장점보다 크기 때문에 적용하지 않았다.

<br/>

## 추가하고 싶은 기능이 있나

### 네트워크 상황이 안좋을 때 이미지 처리

  - 회색의 빈 화면을 로드하고 이미지가 들어오면 교체
  - 저화질로 압축한 파일을 먼저 보여준 후 네트워크 상황이 나아지면 고화질로 변경


### 캐싱 전략

  - 메인 페이지 접속 이후 상세 페이지 접근시 캐시 참조


### 스크롤 위치 저장

  - 다른 페이지 갔다가 돌아왔을 때 스크롤 위치가 보존

### 그래프 형태의 RDB

<br/>

## 힘들었던 점(기술)

### 무한 스크롤

스크롤 요청이 일어날 지점들을 observer를 등록하여 스크롤이 해당 지점에 도달하는지를 지켜본다. 끝에 도달할 경우 추가 데이터를 요청하고 응답이 오면 이를 현재의 리스트 끝에 붙인다.여기서 주의할 점은 요청을 중복해서 보내면 안된다는 것이다. 이를 위해 하나의 요청을 보낸 후엔 응답이 오기 까지 loading state를  true로 설정한다.

#### 문제

문제가 발생하는 타이밍은 응답이 온 직후이다. 응답을 바탕으로 리스트를 재구성하는 작업 또한 비동기로 동작하는데, 따라서 응답과 리스트 업데이트 사이에 딜레이가 약간 발생한다. 이 타이밍에 스크롤을 끝까지 내리면 같은 데이터를 중복해서 요청하게 되는 것이다.

#### 과정

loading state를 리스트가 업데이트 된 직후에 바뀌도록 수정하는 방법이 존재한다. 하지만 현재 구조에서는 클라이언트에서 요청을 보낼 때 아폴로를 사용한다. 아폴로는 자체적으로 loading state를 관리하기 때문에 이를 커스터마이징 하는 것은 힘든 일이었다.

처음 구조에서는 요청을 보내는 스크롤 위치가 여러개였다. 이 때문에 위의 버그가 잦게 발생하였다.

#### 해결

요청을 보내는 즉시 observer를 disconnect한다. 응답이 와서 re-rendering이 될 때, 다시 observer 등록한다. 이렇게 하면 데이터를 두 번이상 보내지 않는다.

위의 방식을 원활히 적용하기 위해선 observer가 하나만 존재하여야 했다. observer가 여러개 존재할 경우 

(요청을 보내는 곳을 한 군데로 고정 => 요청 두개를 빠르게 보낼 경우 위의 문제가 발생할 수 있음. 한군데로 바꾸지 않고 위의 방식을 적용하면, 하나의 요청이 보내지는 즉시 observer 가 모두 해제 되기 때문에 그 다음 요청은 보내질 일이 없음.)

## 힘들었던 점(팀플)

### 공부 시간의 부족

새로운 기술을 많이 도입하다 보니 추가 적인 학습이 다량으로 요구되었다. 하지만 개발 시간이 한정 되어있었기 때문에, 각각의 기술들을 충분히 학습하지 못하고 적용하여 버그가 발생하는 경우가 종종 있었다.

#### 해결

팀원들과 상의하여 주로 맡을 분야를 나누고 서로 가르쳐 주는 방식으로 진행하였다. 나의 경우 클라이언트 전반과 아폴로 상태 관리 및 성능 최적화를 맡았다.

### 팀원과의 방향성 

한정된 시간에 모든 팀원이 원하는 스택과 기술을 적용할 수는 없었다. 팀 회의를 통해 각자 원하는 기술 스택과 그 스택을 소화하기 위한 시간등을 고민해보았다. 이후 이를 바탕으로 기술 스택을 한정하였으며 새롭게 세운 일정표를 멘토님들께 검토 받았다. 

### 최적화에 대한 고민

최적화라는게 기능이 많아지고 복잡해질 때 의미가 있는건데 그래서 지금 단계에서 그걸 고민하는게 맞는지 모르겠다는 고민이 있었다. 이에 대해서는 핵심 기능을 우선시 한 후 버그가 없을 시에 적용하는 것이 좋겠다는 결론 맺었다.



## 백엔드 개발

### 페이지네이션

- 데이터 베이스에 요청

### 질문 1. (large N) 명을 팔로우 하는 경우는 어떻게 처리할 것인가

> 1000명을 팔로우 하고 있는 경우를 가정해보자. 우선 팔로우 테이블 에서 내가 팔로우하고 있는 사용자의 리스트를 구해야 한다. 그리고 게시글 테이블에서 위 리스트를 where 절에 넣어서 검색을 한다. 전체 사용자가 지나치게 많아 게시글 테이블에 게시글이 너무 많고 내가 팔로우 하고 있는 사람들이 너무 많을 경우에 과부하가 걸리지 않는가?

### 답

> 해결 하기 힘든 문제이다. 실제로 페이스북은 이 문제를 해결하기 위해서 운영체제를 튜닝해서 사용하고 있다. 이러한 문제는 사용자가 10만명 이상 있어야 겨우 지연이 보이는 수준이기 때문에 현 단계에서는 고려할 필요가 없다.

> 사용자가 늘어나면 이런부분을 튜닝을 하겠다. 학습을 한다. 구현은 현재 상황에 맞게 나중에 스트레스가 올라갔을 때 나중에 유저가 많아지는 상황에 당면했을 때 어떻게 대응할건지 어떻게 문제를 해결할 것인지를 고민하고 해당 내용을 포함하라.

### 질문 2. 페이지네이션 질문

> limit와 offset을 지정해서 페이지네이션을 할 때에, 매번 orderby를 이용하여 테이블을 정렬하는가. 

### 답

> orderby에 인덱스를 설정해놓으면, 매번 정렬을 하는 것이 아니라 (엄밀히 말하면 맞는 것 같지만) 인덱스를 타는 것이기 때문에 큰 부담이 되지 않는다.

### 질문 3. 페이지네이션 원리

> limit를 사용할 경우, limit 만큼의 데이터만 찾고 search를 끝내는가

### 답

> 그렇다. 그렇기 때문에 더더욱 order by를 하면 안된다. order by를 할 경우에는 DB 전체의 테이블을 정렬하고 상단부터 limit개 만큼 가져오기 때문에 그 비용이 지나치다. order by가 들어가 있으면 기본적으로 잘못된 쿼리이다.

>  레코드를 쌓으면 시간의 역순으로 쌓이게 되어있다. 이를 이용하여 할 수 도 있다. 하지만 실제로 DB에 값이 들어갈 때 정확한 시간 순서가 보장되지 않을 수도 있다. 

> 하지만 index를 사용하면 괜찮다. index를 사용하여 order by를 하면, 이미 정렬이 되어 있기 때문에 전체 테이블을 검색하지 않는다.

> If) User 테이블에 Index를 설정하는데 User를 검색할 때에는 `길이순`으로 검색하고 싶을 경우에는 Index가 의미가 있을까? => 의미가 없다.

> If2) User가 사전 순서대로 b tree가 들어갈 때에 검색을 역 사전 순으로 하면  index의 효율성이 유지가 되는가? -=> 그렇다.

쿼리플랜 explain db에 엑세스하는 건 무조건 적어야한다. 인덱스를 걸어야한다. 보는게 중요한 서비스이기떄문에 읽기 최적화에 집중해야한다. 캐시를 하는게 좋다. 레디스랑 mysql 속도가 1000배쯤 차이가 난다. 인메모리 캐시같은걸 잘 활용해서 하면 순식간에 가져올 수 있다.

### 질문 4. 밀리 세컨드까지 같을 경우에 순서를 어떻게 보장할 것인가.

### order by를 두 번하는 방법

>  updatedAt만으로 정렬을 하는 것이 아니라 ['updatedAt', 'PK']로 정렬을 하는 방법.
>  정렬을 두번 한다는 점에서 비효율적이다.

### cursor에 lte를 적용하는 경우

>  현재 커서에서는 로드된 마지막 게시글의 작성 시간이 들어가있다. 현 방식은 cursor 값에 Op.lt를 적용하여 마지막 게시글 이후의 게시글 중에서 10개를 가져온다. 즉 모든 게시글의 작성 시간 값은 unique 하다는 것을 전제로 한다.
>
>  lte 방식을 이용하면, 마지막 게시글의 작성 시간이하(해당 시간 포함 및 이전 시간) 포함한 게시글들을 요청한다. 클라이언트에서 겹치는 게시글을 제외하고 렌더링 한다. 이 방식은 시간이 겹치는 게시글이 limit개 미만일 경우에만 정상 작동한다. limit개가 넘을 경우 cursor 값이 변경되지 않아 해당 시간의 게시글만 무한히 요청된다.